{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22996,
     "status": "ok",
     "timestamp": 1587833070966,
     "user": {
      "displayName": "COVID19 Segmentation",
      "photoUrl": "",
      "userId": "12351684125312759490"
     },
     "user_tz": -330
    },
    "id": "L3oxAJU9XI8A",
    "outputId": "960b48d1-8ccf-4222-eec8-4392df13eba6"
   },
   "outputs": [],
   "source": [
    "#Mount notebook to google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1153,
     "status": "ok",
     "timestamp": 1587833072844,
     "user": {
      "displayName": "COVID19 Segmentation",
      "photoUrl": "",
      "userId": "12351684125312759490"
     },
     "user_tz": -330
    },
    "id": "WwG7zC4SCKng",
    "outputId": "0ebc9b1c-505a-456e-b3df-e7eb918f8700"
   },
   "outputs": [],
   "source": [
    "%cd drive/My Drive/keras-u-net-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 211828,
     "status": "ok",
     "timestamp": 1587833285522,
     "user": {
      "displayName": "COVID19 Segmentation",
      "photoUrl": "",
      "userId": "12351684125312759490"
     },
     "user_tz": -330
    },
    "id": "zTuR6l89qb5N",
    "outputId": "3637dfcc-ecc6-4b41-9e72-e24551ecb100"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#Import proper libraries\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from skimage.io import imsave, imread\n",
    "from PIL import Image\n",
    "data_path = 'data/'\n",
    "#Set Image width and height\n",
    "image_rows = 512\n",
    "image_cols = 512\n",
    "\n",
    "#Function to create the training data\n",
    "def create_train_data():\n",
    "    train_data_path = os.path.join(data_path, 'train/Image PP/')\n",
    "    train_data_Label_path = os.path.join(data_path, 'train/Label/')\n",
    "    images = os.listdir(train_data_path)\n",
    "    total = len(images)\n",
    "\n",
    "    imgs = np.ndarray((total, image_rows, image_cols), dtype=np.uint8)\n",
    "    imgs_mask = np.ndarray((total, image_rows, image_cols), dtype=np.uint8)\n",
    "\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print('Creating training images...')\n",
    "    print('-'*30)\n",
    "    for image_name in images:\n",
    "        img = imread(os.path.join(train_data_path, image_name), as_gray=True)\n",
    "        img_mask = imread(os.path.join(train_data_Label_path, image_name), as_gray=True)\n",
    "\n",
    "        img = np.array([img])\n",
    "        img_mask = np.array([img_mask])\n",
    "\n",
    "        imgs[i] = img\n",
    "        imgs_mask[i] = img_mask\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print('Done: {0}/{1} images'.format(i, total))\n",
    "        i += 1\n",
    "    print('Loading done.')\n",
    "\n",
    "    np.save('imgs_train.npy', imgs)\n",
    "    np.save('imgs_mask_train.npy', imgs_mask)\n",
    "    print('Saving to .npy files done.')\n",
    "\n",
    "#Function to laod the training data\n",
    "def load_train_data():\n",
    "    imgs_train = np.load('imgs_train.npy')\n",
    "    imgs_mask_train = np.load('imgs_mask_train.npy')\n",
    "    return imgs_train, imgs_mask_train\n",
    "\n",
    "#Function to create the testing data\n",
    "def create_test_data():\n",
    "    train_data_path = os.path.join(data_path, 'test/Image PP')\n",
    "    images = os.listdir(train_data_path)\n",
    "    total = len(images)\n",
    "\n",
    "    imgs = np.ndarray((total, image_rows, image_cols), dtype=np.uint8)\n",
    "    imgs_id = np.ndarray((total, ), dtype=np.int32)\n",
    "\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print('Creating test images...')\n",
    "    print('-'*30)\n",
    "    for image_name in images:\n",
    "        img_id = int(image_name.split('.')[0])\n",
    "        img = imread(os.path.join(train_data_path, image_name), as_gray=True)\n",
    "\n",
    "        img = np.array([img])\n",
    "\n",
    "        imgs[i] = img\n",
    "        imgs_id[i] = img_id\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Done: {0}/{1} images'.format(i, total))\n",
    "        i += 1\n",
    "    print('Loading done.')\n",
    "\n",
    "    np.save('imgs_test.npy', imgs)\n",
    "    np.save('imgs_id_test.npy', imgs_id)\n",
    "    print('Saving to .npy files done.')\n",
    "\n",
    "#Function to load the testing data\n",
    "def load_test_data():\n",
    "    imgs_test = np.load('imgs_test.npy')\n",
    "    imgs_id = np.load('imgs_id_test.npy')\n",
    "    return imgs_test, imgs_id\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_train_data()\n",
    "    create_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5585654,
     "status": "ok",
     "timestamp": 1587839460104,
     "user": {
      "displayName": "COVID19 Segmentation",
      "photoUrl": "",
      "userId": "12351684125312759490"
     },
     "user_tz": -330
    },
    "id": "l-Ke6uUKqda4",
    "outputId": "d10cc9fb-ea9e-4a54-8df2-17130edee243"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#Import proper Libraries\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "from keras.models import Sequential,Model  \n",
    "from keras.layers import Conv2D,MaxPooling2D,UpSampling2D,BatchNormalization,Reshape,Permute,Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import losses,metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_preparation import load_train_data, load_test_data\n",
    "\n",
    "K.set_image_data_format('channels_last')  # TF dimension ordering in this code\n",
    "#Set Image width and height\n",
    "img_rows = 256\n",
    "img_cols = 256\n",
    "n_label=2\n",
    "smooth = 1.\n",
    "\n",
    "#Function to calculate the Dice Coefficient\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "#Function to calculate the Dice Coefficient Loss\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return dice_coef(y_true, y_pred)\n",
    "\n",
    "#Function to define the SegNet Model\n",
    "def SegNet(pretrained_weights=None):\n",
    "    model = Sequential()  \n",
    "    #encoder  \n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=(img_rows,img_cols,1),padding='same',activation='relu',data_format='channels_last'))\n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))  \n",
    "    #(128,128)  \n",
    "    model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    #(64,64)  \n",
    "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "    #(32,32)  \n",
    "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "    #(16,16)  \n",
    "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "    #(8,8)  \n",
    "    #decoder  \n",
    "    model.add(UpSampling2D(size=(2,2)))  \n",
    "    #(16,16)  \n",
    "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(UpSampling2D(size=(2, 2)))  \n",
    "    #(32,32)  \n",
    "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(UpSampling2D(size=(2, 2)))  \n",
    "    #(64,64)  \n",
    "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(UpSampling2D(size=(2, 2)))  \n",
    "    #(128,128)  \n",
    "    model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(UpSampling2D(size=(2, 2)))  \n",
    "    #(256,256)  \n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), input_shape=(img_rows, img_cols,3), padding='same', activation='relu',data_format='channels_last'))\n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu'))  \n",
    "    model.add(BatchNormalization())  \n",
    "    model.add(Conv2D(1, (1, 1), strides=(1, 1), padding='same'))  \n",
    "    #model.add(Reshape((img_rows,img_cols,n_label)))\n",
    "    #model.add(Permute((2,1)))  \n",
    "    model.add(Activation('sigmoid'))\n",
    "    #Model is compiled using Adam, Binary Cross Entropy and Dice Coefficient, accuracy and MAE metrics\n",
    "    model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=[dice_coef,'accuracy',tf.keras.metrics.MeanIoU(num_classes=2),metrics.mae])  \n",
    "    model.summary()  \n",
    "    \n",
    "    if (pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "#Function to resize the images for predictions\n",
    "def preprocess(imgs):\n",
    "    imgs_p = np.ndarray((imgs.shape[0], img_rows, img_cols), dtype=np.uint8)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_p[i] = resize(imgs[i], (img_cols, img_rows), preserve_range=True)\n",
    "\n",
    "    imgs_p = imgs_p[..., np.newaxis]\n",
    "    return imgs_p\n",
    "\n",
    "#Main function to train the model and predict the masks. \n",
    "def train_and_predict():\n",
    "    print('Loading and preprocessing train data...')\n",
    "    imgs_train, imgs_mask_train = load_train_data()\n",
    "\n",
    "    imgs_train = preprocess(imgs_train)\n",
    "    imgs_mask_train = preprocess(imgs_mask_train)\n",
    "\n",
    "    imgs_train = imgs_train.astype('float32')\n",
    "    mean = np.mean(imgs_train)  # mean for data centering\n",
    "    std = np.std(imgs_train)  # std for data normalization\n",
    "\n",
    "    imgs_train -= mean\n",
    "    imgs_train /= std\n",
    "\n",
    "    imgs_mask_train = imgs_mask_train.astype('float32')\n",
    "    imgs_mask_train /= 255.  # scale masks to [0, 1]\n",
    "\n",
    "    #model = SegNet('unet_weights_150_Img_SegNet.h5') #If you are training again, uncomment  to load the pre-training model\n",
    "    model = SegNet()\n",
    "    #Model checkpoints are saved and validation loss is monitored\n",
    "    model_checkpoint = ModelCheckpoint('unet_weights_150_Img_SegNet.h5', monitor='val_loss', save_best_only=True)\n",
    "    print('Fitting model...')\n",
    "    hist=model.fit(imgs_train, imgs_mask_train, batch_size=16, nb_epoch=1000, verbose=1, shuffle=True,\n",
    "              validation_split=0.2,\n",
    "              callbacks=[model_checkpoint])\n",
    "    print(model.summary())\n",
    "\n",
    "    imgs_test, imgs_id_test = load_test_data()\n",
    "    imgs_test = preprocess(imgs_test)\n",
    "    imgs_test = imgs_test.astype('float32')\n",
    "    mean=np.mean(imgs_test)\n",
    "    std=np.std(imgs_test)\n",
    "    imgs_test -= mean\n",
    "    imgs_test /= std\n",
    "\n",
    "    model.load_weights('unet_weights_150_Img_SegNet.h5')\n",
    "\n",
    "    print('Predicting masks on test data...')\n",
    "\n",
    "    imgs_mask_test = model.predict(imgs_test, verbose=1)\n",
    "    np.save('imgs_mask_test.npy', imgs_mask_test)\n",
    "    pred_dir = 'preds_SegNet'\n",
    "    \n",
    "    #After predictions, the mask images are saved in the preds_SegNet folder\n",
    "    if not os.path.exists(pred_dir):\n",
    "      os.mkdir(pred_dir)\n",
    "\n",
    "    for image, image_id in zip(imgs_mask_test, imgs_id_test):\n",
    "      image = (image[:, :, 0] * 255.).astype(np.uint8)\n",
    "      imsave(os.path.join(pred_dir, str(image_id) + '_pred.png'), image)\n",
    "    \n",
    "    #History of the model is then plotted to show the various curves\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pickle\n",
    "    model.load_weights('unet_weights_150_Img_SegNet.h5')\n",
    "    l_s=plt.plot(hist.history['loss'], color='b')\n",
    "    vl_s=plt.plot(hist.history['val_loss'], color='r')\n",
    "    plt.title('Loss Curve')\n",
    "    pickle.dump(l_s, open('Loss_Seg.fig.pickle', 'wb')) # This is for Python 3 - py2 may need `file` instead of `open`\n",
    "    pickle.dump(vl_s, open('Val_Loss_Seg.fig.pickle', 'wb')) # This is for Python 3 - py2 may need `file` instead of `open`\n",
    "    plt.show()\n",
    "\n",
    "    d_s=plt.plot(hist.history['dice_coef'], color='b')\n",
    "    vd_s=plt.plot(hist.history['val_dice_coef'], color='r')\n",
    "    plt.title('Dice Coefficient Curve')\n",
    "    pickle.dump(d_s, open('Dice_Seg.fig.pickle', 'wb')) # This is for Python 3 - py2 may need `file` instead of `open`\n",
    "    pickle.dump(vd_s, open('Val_Dice_Seg.fig.pickle', 'wb')) # This is for Python 3 - py2 may need `file` instead of `open`\n",
    "    plt.show()\n",
    "\n",
    "    a_s=plt.plot(hist.history['accuracy'], color='b')\n",
    "    va_s=plt.plot(hist.history['val_accuracy'], color='r')\n",
    "    plt.title('Accuracy Curve')\n",
    "    pickle.dump(a_s, open('Acc_Seg.fig.pickle', 'wb')) # This is for Python 3 - py2 may need `file` instead of `open`\n",
    "    pickle.dump(va_s, open('Val_Acc_Seg.fig.pickle', 'wb')) # This is for Python 3 - py2 may need `file` instead of `open`\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_and_predict()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "SegNet_Seg_COVID_Code.ipynb",
   "provenance": [
    {
     "file_id": "1YM8N4A1CscUzjrD5DeQ29XlHN-ik3qzy",
     "timestamp": 1587616945013
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
