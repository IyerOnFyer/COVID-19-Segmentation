{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26589,
     "status": "ok",
     "timestamp": 1587965829973,
     "user": {
      "displayName": "COVID19 Segmentation",
      "photoUrl": "",
      "userId": "12351684125312759490"
     },
     "user_tz": -330
    },
    "id": "L3oxAJU9XI8A",
    "outputId": "6154c763-fe3a-4b60-ccea-7eb226e0e61e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1587965831675,
     "user": {
      "displayName": "COVID19 Segmentation",
      "photoUrl": "",
      "userId": "12351684125312759490"
     },
     "user_tz": -330
    },
    "id": "WwG7zC4SCKng",
    "outputId": "d7f5b4f6-ce8a-4e2e-d554-c3d8045bc449"
   },
   "outputs": [],
   "source": [
    "%cd drive/My Drive/keras-u-net-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 208597,
     "status": "ok",
     "timestamp": 1587966042399,
     "user": {
      "displayName": "COVID19 Segmentation",
      "photoUrl": "",
      "userId": "12351684125312759490"
     },
     "user_tz": -330
    },
    "id": "zTuR6l89qb5N",
    "outputId": "49590afb-4934-470b-8f7c-24a28e3ca5a7"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#Import proper libraries to read the data\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from skimage.io import imsave, imread\n",
    "from PIL import Image\n",
    "data_path = 'data/'\n",
    "#Set the image width and height\n",
    "image_rows = 512\n",
    "image_cols = 512\n",
    "#Define function to create data for training set\n",
    "\n",
    "def create_train_data():\n",
    "    train_data_path = os.path.join(data_path, 'train/Image PP/')\n",
    "    train_data_Label_path = os.path.join(data_path, 'train/Label/')\n",
    "    images = os.listdir(train_data_path)\n",
    "    total = len(images)\n",
    "\n",
    "    imgs = np.ndarray((total, image_rows, image_cols), dtype=np.uint8)\n",
    "    imgs_mask = np.ndarray((total, image_rows, image_cols), dtype=np.uint8)\n",
    "\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print('Creating training images...')\n",
    "    print('-'*30)\n",
    "    for image_name in images:\n",
    "        img = imread(os.path.join(train_data_path, image_name), as_gray=True)\n",
    "        img_mask = imread(os.path.join(train_data_Label_path, image_name), as_gray=True)\n",
    "\n",
    "        img = np.array([img])\n",
    "        img_mask = np.array([img_mask])\n",
    "\n",
    "        imgs[i] = img\n",
    "        imgs_mask[i] = img_mask\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print('Done: {0}/{1} images'.format(i, total))\n",
    "        i += 1\n",
    "    print('Loading done.')\n",
    "\n",
    "    np.save('imgs_train.npy', imgs)\n",
    "    np.save('imgs_mask_train.npy', imgs_mask)\n",
    "    print('Saving to .npy files done.')\n",
    "\n",
    "#Define function to load the training set\n",
    "def load_train_data():\n",
    "    imgs_train = np.load('imgs_train.npy')\n",
    "    imgs_mask_train = np.load('imgs_mask_train.npy')\n",
    "    return imgs_train, imgs_mask_train\n",
    "\n",
    "#Define function to create testing dataset\n",
    "def create_test_data():\n",
    "    train_data_path = os.path.join(data_path, 'test/Image PP')\n",
    "    images = os.listdir(train_data_path)\n",
    "    total = len(images)\n",
    "\n",
    "    imgs = np.ndarray((total, image_rows, image_cols), dtype=np.uint8)\n",
    "    imgs_id = np.ndarray((total, ), dtype=np.int32)\n",
    "\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print('Creating test images...')\n",
    "    print('-'*30)\n",
    "    for image_name in images:\n",
    "        img_id = int(image_name.split('.')[0])\n",
    "        img = imread(os.path.join(train_data_path, image_name), as_gray=True)\n",
    "\n",
    "        img = np.array([img])\n",
    "\n",
    "        imgs[i] = img\n",
    "        imgs_id[i] = img_id\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Done: {0}/{1} images'.format(i, total))\n",
    "        i += 1\n",
    "    print('Loading done.')\n",
    "\n",
    "    np.save('imgs_test.npy', imgs)\n",
    "    np.save('imgs_id_test.npy', imgs_id)\n",
    "    print('Saving to .npy files done.')\n",
    "#Define function to load testing data\n",
    "def load_test_data():\n",
    "    imgs_test = np.load('imgs_test.npy')\n",
    "    imgs_id = np.load('imgs_id_test.npy')\n",
    "    return imgs_test, imgs_id\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_train_data()\n",
    "    create_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5283248,
     "status": "ok",
     "timestamp": 1587971432550,
     "user": {
      "displayName": "COVID19 Segmentation",
      "photoUrl": "",
      "userId": "12351684125312759490"
     },
     "user_tz": -330
    },
    "id": "l-Ke6uUKqda4",
    "outputId": "397e79c7-3a1f-463f-f564-32ba57e0502b"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#Import proper libraries\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import losses,metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_preparation import load_train_data, load_test_data\n",
    "\n",
    "K.set_image_data_format('channels_last')  # TF dimension ordering in this code\n",
    "\n",
    "img_rows = 512\n",
    "img_cols = 512\n",
    "\n",
    "smooth = 1.\n",
    "\n",
    "#Define function for dice coefficient and dice coeff loss\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return dice_coef(y_true, y_pred)\n",
    "\n",
    "#Define function for UNet Model\n",
    "def get_unet(pretrained_weights=None):\n",
    "    inputs = Input((img_rows, img_cols, 1))\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv10])\n",
    "    #Compile model using Adam optimiser and appropriate loss function and metrics\n",
    "    model.compile(optimizer=Adam(lr=1e-5), loss='binary_crossentropy', metrics=[dice_coef,'acc', tf.keras.metrics.MeanIoU(num_classes=2),metrics.mae])\n",
    "    #model.compile(optimizer=Adam(lr=1e-5), loss='binary_crossentropy', metrics=[metrics.binary_accuracy,keras_metrics.binary_precision(),keras_metrics.binary_recall()])\n",
    "    #model.compile(optimizer=Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    if (pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model\n",
    "#Define function to preprocess the images before predicting the test data\n",
    "def preprocess(imgs):\n",
    "    imgs_p = np.ndarray((imgs.shape[0], img_rows, img_cols), dtype=np.uint8)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_p[i] = resize(imgs[i], (img_cols, img_rows), preserve_range=True)\n",
    "\n",
    "    imgs_p = imgs_p[..., np.newaxis]\n",
    "    return imgs_p\n",
    "\n",
    "#The main function to train the model and predict the test data\n",
    "def train_and_predict():\n",
    "    print('Loading and preprocessing train data...')\n",
    "    imgs_train, imgs_mask_train = load_train_data()\n",
    "\n",
    "    imgs_train = preprocess(imgs_train)\n",
    "    imgs_mask_train = preprocess(imgs_mask_train)\n",
    "\n",
    "    imgs_train = imgs_train.astype('float32')\n",
    "    mean = np.mean(imgs_train)  # mean for data centering\n",
    "    std = np.std(imgs_train)  # std for data normalization\n",
    "\n",
    "    imgs_train -= mean\n",
    "    imgs_train /= std\n",
    "\n",
    "    imgs_mask_train = imgs_mask_train.astype('float32')\n",
    "    imgs_mask_train /= 255.  # scale masks to [0, 1]\n",
    "\n",
    "    #model = get_unet('unet_weights_150_Img_PP.h5') #If you are training again, use this line to load the pre-training model\n",
    "    model = get_unet()\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint('unet_weights_150_Img_PP.h5', monitor='val_loss', save_best_only=True)\n",
    "    print('Fitting model...')\n",
    "    hist=model.fit(imgs_train, imgs_mask_train, batch_size=8, nb_epoch=1000, verbose=1, shuffle=True,\n",
    "              validation_split=0.2,\n",
    "              callbacks=[model_checkpoint])\n",
    "    print(model.summary())\n",
    "\n",
    "    imgs_test, imgs_id_test = load_test_data()\n",
    "    imgs_test = preprocess(imgs_test)\n",
    "    imgs_test = imgs_test.astype('float32')\n",
    "    mean=np.mean(imgs_test)\n",
    "    std=np.std(imgs_test)\n",
    "    imgs_test -= mean\n",
    "    imgs_test /= std\n",
    "\n",
    "    model.load_weights('unet_weights_150_Img_PP.h5')\n",
    "\n",
    "    print('Predicting masks on test data...')\n",
    "\n",
    "    imgs_mask_test = model.predict(imgs_test, verbose=1)\n",
    "    np.save('imgs_mask_test.npy', imgs_mask_test)\n",
    "    pred_dir = 'preds_UNet'\n",
    "\n",
    "    if not os.path.exists(pred_dir):\n",
    "      os.mkdir(pred_dir)\n",
    "\n",
    "    for image, image_id in zip(imgs_mask_test, imgs_id_test):\n",
    "      image = (image[:, :, 0] * 255.).astype(np.uint8)\n",
    "      imsave(os.path.join(pred_dir, str(image_id) + '_pred.png'), image)\n",
    "    #Load the weights and plot the history of the model\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pickle\n",
    "    model.load_weights('unet_weights_150_Img_PP.h5')\n",
    "    \n",
    "    l_u=plt.plot(hist.history['loss'], color='b')\n",
    "    vl_u=plt.plot(hist.history['val_loss'], color='r')\n",
    "    plt.title('Loss Curve')\n",
    "    pickle.dump(l_u, open('Loss_UN.fig.pickle', 'wb')) # This is for Python 3 - py2 may need `file` instead of `open`\n",
    "    pickle.dump(vl_u, open('Val_Loss_UN.fig.pickle', 'wb')) # This is for Python 3 - py2 may need `file` instead of `open`\n",
    "    plt.show()\n",
    "\n",
    "    d_u=plt.plot(hist.history['dice_coef'], color='b')\n",
    "    vd_u=plt.plot(hist.history['val_dice_coef'], color='r')\n",
    "    plt.title('Dice Coefficient Curve')\n",
    "    pickle.dump(d_u, open('Dice_UN.fig.pickle', 'wb')) # This is for Python 3 - py2 may need `file` instead of `open`\n",
    "    pickle.dump(vd_u, open('Val_Dice_UN.fig.pickle', 'wb')) # This is for Python 3 - py2 may need `file` instead of `open`\n",
    "    plt.show()\n",
    "\n",
    "    a_u=plt.plot(hist.history['acc'], color='b')\n",
    "    va_u=plt.plot(hist.history['val_acc'], color='r')\n",
    "    plt.title('Accuracy Curve')\n",
    "    pickle.dump(a_u, open('Acc_UN.fig.pickle', 'wb')) # This is for Python 3 - py2 may need `file` instead of `open`\n",
    "    pickle.dump(va_u, open('Val_Acc_UN.fig.pickle', 'wb')) # This is for Python 3 - py2 may need `file` instead of `open`\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_and_predict()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "UNet_Seg_COVID_Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
